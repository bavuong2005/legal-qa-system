# retriever_custom.py
# -*- coding: utf-8 -*-
"""
Custom Retriever v·ªõi BM25+pyvi + Dense + Reranker
- BM25: rank-bm25 v·ªõi pyvi tokenization (t·ªët h∆°n Weaviate built-in)
- Dense: Weaviate vector search v·ªõi Alibaba-NLP/gte-multilingual-base
- Reranker: BAAI/bge-reranker-v2-m3
- Hybrid: alpha tuning based on query patterns
"""

import os
import re
import json
import pickle
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple

import torch
import weaviate
from pyvi import ViTokenizer
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer, CrossEncoder

# ---------------- ENV SETUP ----------------
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
torch.backends.mps.is_available = lambda: False
torch.set_num_threads(1)

# ---------------- CONFIG ----------------
BM25_INDEX_FILE = Path("bm25_index.pkl")

INITIAL_K = 20  # L·∫•y 20 candidates tr∆∞·ªõc khi rerank
FINAL_K = 5     # Top 5 sau rerank

# ---------------- PATTERNS ----------------
LEGAL_HINT_RE = re.compile(r"\b(Ch∆∞∆°ng|M·ª•c|ƒêi·ªÅu|Kho·∫£n|ƒêi·ªÉm)\s+[IVXLC\d]+", re.IGNORECASE)
NUMERIC_INFO_RE = re.compile(r"\b(\d+)\s*(gi·ªù|km/h|tri·ªáu|ngh√¨n|ƒë·ªìng|l·∫ßn|ng√†y|th√°ng|nƒÉm|%|ph·∫ßn trƒÉm|cm3|cc|t·∫•n|km|m|kW|ƒëi·ªÉm|gi·∫•y ph√©p l√°i xe)\b", re.IGNORECASE)

def tune_alpha(query: str, base_alpha: float = 0.55) -> float:
    """Dynamic alpha tuning based on query pattern"""
    alpha = base_alpha
    
    if LEGAL_HINT_RE.search(query):
        alpha = max(0.30, base_alpha - 0.25)   # Thi√™n keyword
    elif NUMERIC_INFO_RE.search(query):
        alpha = max(0.40, base_alpha - 0.15)
    else:
        alpha = min(0.75, base_alpha + 0.20)   # Thi√™n semantic
    
    return alpha

# ---------------- LOAD MODELS ----------------
print("üîπ Loading embedding model...")
emb_model = SentenceTransformer("Alibaba-NLP/gte-multilingual-base", device="cpu", trust_remote_code=True)
print("‚úì Embedding model loaded")

print("üîπ Loading reranker model...")
reranker = CrossEncoder("BAAI/bge-reranker-v2-m3", device="cpu")
print("‚úì Reranker loaded")

print("üåê Connecting to Weaviate...")
client = weaviate.connect_to_local()
weaviate_collection = client.collections.get("LawChunks")
print("‚úì Connected to Weaviate")

# ---------------- BUILD/LOAD BM25 INDEX ----------------
def load_chunks_from_weaviate():
    """Load chunks t·ª´ Weaviate (lu√¥n fresh, kh√¥ng cache)"""
    resp = weaviate_collection.query.fetch_objects(
        limit=10000,
        return_properties=[
            "law", "chapter", "section", "article_no", "article_title",
            "clause_no", "point", "clause_head", "text",
            "enriched_text", "display_citation"
        ]
    )
    
    chunks = []
    for obj in resp.objects:
        chunks.append(obj.properties)
    
    return chunks

def build_bm25_index(chunks):
    """Build BM25 index v·ªõi pyvi tokenization"""
    print("\nüî® Building BM25 index v·ªõi pyvi...")
    
    tokenized_corpus = []
    for c in chunks:
        # Combine fields for BM25 (gi·ªëng eval code)
        fields = [
            c.get("article_no", ""),
            c.get("article_title", ""),
            c.get("clause_no", ""),
            c.get("point", ""),
            c.get("clause_head", ""),
            c.get("text", ""),
        ]
        combined = " ".join([f for f in fields if f])
        
        # Tokenize v·ªõi pyvi
        tokenized = ViTokenizer.tokenize(combined)
        tokenized_corpus.append(tokenized.split())
    
    bm25 = BM25Okapi(tokenized_corpus)
    
    # Cache ch·ªâ BM25 object (kh√¥ng cache chunks)
    with open(BM25_INDEX_FILE, "wb") as f:
        pickle.dump(bm25, f)
    
    print(f"‚úì BM25 index built: {len(chunks)} chunks")
    return bm25

# Load chunks (lu√¥n fresh t·ª´ Weaviate)
print("üìÇ Loading chunks from Weaviate...")
chunks_cache = load_chunks_from_weaviate()
print(f"‚úì Loaded {len(chunks_cache)} chunks")

# Load or build BM25 index
if BM25_INDEX_FILE.exists():
    print("üìÇ Loading cached BM25 index...")
    with open(BM25_INDEX_FILE, "rb") as f:
        bm25_index = pickle.load(f)
    print(f"‚úì BM25 index loaded")
else:
    bm25_index = build_bm25_index(chunks_cache)

# ---------------- RETRIEVAL FUNCTIONS ----------------
def retrieve_bm25(query: str, k: int) -> Tuple[List[int], List[float]]:
    """BM25 retrieval v·ªõi pyvi"""
    tokenized_query = ViTokenizer.tokenize(query).split()
    scores = bm25_index.get_scores(tokenized_query)
    top_indices = np.argsort(-scores)[:k]
    top_scores = scores[top_indices]
    return top_indices.tolist(), top_scores.tolist()

def retrieve_dense(query: str, k: int) -> List[Dict]:
    """Dense vector search t·ª´ Weaviate"""
    q_vec = emb_model.encode([query], normalize_embeddings=True)[0]
    
    resp = weaviate_collection.query.near_vector(
        near_vector=q_vec.tolist(),
        limit=k,
        return_metadata=["distance"],
        return_properties=[
            "law", "chapter", "section", "article_no", "article_title",
            "clause_no", "point", "clause_head", "text",
            "enriched_text", "display_citation"
        ]
    )
    
    results = []
    for obj in resp.objects:
        p = obj.properties
        # Convert distance to similarity score (1 - distance)
        score = 1.0 - obj.metadata.distance if obj.metadata.distance else 0.0
        results.append({
            "props": p,
            "score": score
        })
    
    return results

def retrieve_hybrid(query: str, alpha: float, k: int) -> List[Dict]:
    """
    Hybrid: alpha * dense + (1-alpha) * bm25
    alpha=0 ‚Üí pure keyword, alpha=1 ‚Üí pure vector
    """
    # BM25 scores
    bm25_indices, bm25_scores = retrieve_bm25(query, k)
    bm25_scores_norm = np.array(bm25_scores) / (np.max(bm25_scores) + 1e-8)
    
    # Dense scores
    dense_results = retrieve_dense(query, k)
    
    # Merge scores by chunk
    # Create mapping: bm25 chunk ‚Üí score
    bm25_score_map = {}
    for idx, score in zip(bm25_indices, bm25_scores_norm):
        chunk = chunks_cache[idx]
        key = chunk.get("display_citation", "")
        bm25_score_map[key] = score
    
    # Combine
    combined = []
    for dense_res in dense_results:
        citation = dense_res["props"].get("display_citation", "")
        dense_score = dense_res["score"]
        bm25_score = bm25_score_map.get(citation, 0.0)
        
        hybrid_score = alpha * dense_score + (1 - alpha) * bm25_score
        combined.append({
            "props": dense_res["props"],
            "score": hybrid_score
        })
    
    # Add BM25-only results (not in dense top-k)
    dense_citations = {r["props"].get("display_citation", "") for r in dense_results}
    for idx in bm25_indices:
        chunk = chunks_cache[idx]
        citation = chunk.get("display_citation", "")
        if citation not in dense_citations:
            bm25_score = bm25_score_map.get(citation, 0.0)
            hybrid_score = (1 - alpha) * bm25_score
            combined.append({
                "props": chunk,
                "score": hybrid_score
            })
    
    # Sort by hybrid score
    combined.sort(key=lambda x: x["score"], reverse=True)
    return combined[:k]

def rerank(query: str, candidates: List[Dict], final_k: int) -> List[Dict]:
    """Rerank v·ªõi CrossEncoder"""
    if not candidates:
        return []
    
    # Prepare texts
    texts = [c["props"].get("enriched_text", "") for c in candidates]
    pairs = [[query, text] for text in texts]
    
    # Rerank
    scores = reranker.predict(pairs)
    
    # Sort
    for i, c in enumerate(candidates):
        c["rerank_score"] = float(scores[i])
    
    candidates.sort(key=lambda x: x["rerank_score"], reverse=True)
    return candidates[:final_k]

# ---------------- MAIN RETRIEVE FUNCTION ----------------
def retrieve(question: str, k: int = 5) -> Tuple[str, List[str]]:
    """
    Main retrieval function
    Returns: (context_text, sources_list)
    """
    # Ensure k is integer
    k = int(k) if k else 5
    
    # Dynamic alpha tuning
    alpha = tune_alpha(question, base_alpha=0.55)
    
    # Hybrid search
    candidates = retrieve_hybrid(question, alpha, INITIAL_K)
    
    # Rerank
    top_results = rerank(question, candidates, k)
    
    # Format context for LLM (gi·ªëng retriever.py)
    contexts = []
    sources = []
    
    for res in top_results:
        p = res["props"]
        
        law = (p.get("law") or "").strip()
        chapter = (p.get("chapter") or "").strip()
        section = (p.get("section") or "").strip()
        article_no = (p.get("article_no") or "").strip()
        article_title = (p.get("article_title") or "").strip()
        clause_no = (p.get("clause_no") or "")
        clause_head = (p.get("clause_head") or "").strip()
        point = (p.get("point") or "").strip()
        body_for_ctx = (p.get("text") or "").strip()
        display_citation = (p.get("display_citation") or "").strip()
        
        lines = []
        
        # Add citation reference at the top
        if display_citation:
            lines.append(f"[CƒÉn c·ª©: {display_citation}]")
        
        # Lu·∫≠t / Ch∆∞∆°ng / M·ª•c / ƒêi·ªÅu
        if law:
            lines.append(law)
        if chapter:
            lines.append(chapter)
        if section:
            lines.append(section)
        if article_no or article_title:
            art_line = f"ƒêi·ªÅu {article_no}".strip()
            if article_title:
                art_line += f". {article_title}"
            lines.append(art_line)
        
        # Ph√¢n bi·ªát: c√≥ ƒêi·ªÉm hay kh√¥ng
        if point:
            # LEAF = ƒêI·ªÇM: c·∫ßn c·∫£ clause_head + text ƒëi·ªÉm
            if clause_no and clause_head:
                lines.append(f"Kho·∫£n {clause_no}. {clause_head}")
            elif clause_no:
                lines.append(f"Kho·∫£n {clause_no}")
            
            lines.append(f"ƒêi·ªÉm {point})")
            
            if body_for_ctx:
                lines.append(body_for_ctx)
        else:
            # LEAF = KHO·∫¢N: ch·ªâ label + text
            if clause_no:
                lines.append(f"Kho·∫£n {clause_no}")
            if body_for_ctx:
                lines.append(body_for_ctx)
        
        ctx_chunk = "\n".join(lines).strip()
        if ctx_chunk:
            contexts.append(ctx_chunk)
        
        # Source citation
        src = p.get("display_citation", "")
        sources.append(f"{src}" if src else "")
    
    context = "\n\n".join(contexts)
    return context, sources

# ---------------- CLEANUP ----------------
import atexit

def cleanup():
    """Close Weaviate connection on exit"""
    try:
        if client:
            client.close()
    except:
        pass

atexit.register(cleanup)



